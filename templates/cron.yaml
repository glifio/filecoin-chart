{{- if .Values.persistence.enabled }}
{{- if .Values.persistence.snapshots.enabled }}
{{- if and .Values.IPFS.enabled .Values.persistence.snapshots.uploadToIpfs.enabled }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-internal-snapshot-creator
spec:
  schedule: {{ .Values.persistence.snapshots.uploadToIpfs.cron | default "0 0 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: "Forbid"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          initContainers:
{{- if eq .Values.persistence.snapshots.uploadToIpfs.export "hot" }}
          - name: exporter
            image: bitnami/kubectl
            command:
            - bash
            - -c
            - until kubectl exec {{ .Release.Name }}-lotus-0 -c {{ .Chart.Name }} -- bash -c 'lotus chain export --tipset @$( lotus chain list --count 50 --format "<height>" | head -n1 ) --recent-stateroots 900 --skip-old-msgs /data/ipfs/lotus-hot.car && mv /data/ipfs/lotus-hot.car /data/ipfs/lotus-new.car'; do echo "Command executed with error, retrying in 60s..." && sleep 60s; done; 
{{- end }}
          - name: sharer
            image: bitnami/kubectl
            command:
            - bash
            - -c
            - until kubectl exec {{ .Release.Name }}-lotus-0 -c filecoin -- bash -c 'while true; do echo "Checking if file is ready..." && if ! [[ `pgrep -x "lotus-shed"` ]] && [ -s "/data/ipfs/lotus-new.car" ]; then echo "File is ready. Exporting..." && break; else echo "File is not ready, sleeping for 60s..." && sleep 60s; fi; done'; do echo "First command executed with error, retrying in 60s..." && sleep 60s; done && kubectl exec {{ .Release.Name }}-lotus-0 -c filecoin-ipfs -- sh -c 'mkdir -p /data/ipfs/snapshots && mv /data/ipfs/lotus-new.car /data/ipfs/snapshots/lotus.car && ipfs pin ls -q --type recursive | xargs ipfs pin rm && ipfs repo gc && ipfs add /data/ipfs/snapshots/lotus.car | cut -d " " -f2 > /data/ipfs/snapshots/snapshot.log && echo "Exported. Exiting..."'
{{- if .Values.persistence.snapshots.uploadToIpfs.shareToGist.enabled }}
          containers:
          - name: gitter
            image: bitnami/kubectl
            securityContext:
              runAsUser: 0
            volumeMounts:
              - name: lotus-secret-vol
                mountPath: "/secret"
            env:
            - name: GIT_SSH_COMMAND
              value: ssh -i /secret/ssh -o IdentitiesOnly=yes
            command:
            - sh
            - -c 
            - echo "Job started. Waiting for log file to be available..." && kubectl exec -t {{ .Release.Name }}-lotus-0 -c filecoin -- bash -c 'while true; do if [ -f "/data/ipfs/snapshots/snapshot.log" ]; then cat /data/ipfs/snapshots/snapshot.log && break; else sleep 5s; fi; done' > /tmp/snapshot.log && echo "Log exported. Check if it is not empty..." && if [ -s /tmp/snapshot.log ]; then echo "Not empty. Installing git on machine..." && apt update -y && apt install git -y && echo "Installed. Fetching gist.github.com SSH keys..." && mkdir -p /root/.ssh && ssh-keyscan -t rsa gist.github.com >> /root/.ssh/known_hosts && echo "Fetched. Cloning repo..." && git clone {{ .Values.persistence.snapshots.uploadToIpfs.shareToGist.address }} /tmp/snapshots && echo "Cloned. Updating data..." && cp /tmp/snapshot.log /tmp/snapshots/snapshot.log && cd /tmp/snapshots && echo "Updated. Configuring git..." && git config user.name "{{ .Values.persistence.snapshots.uploadToIpfs.shareToGist.authorName }}" && git config user.email "{{ .Values.persistence.snapshots.uploadToIpfs.shareToGist.authorEmail }}" && echo "Configured. Commiting data..." && git commit --allow-empty -a -m "new export" && echo "Commited. Pushing data..." && git push && echo "Pushed. Removing log file..." && kubectl exec {{ .Release.Name }}-lotus-0 -c filecoin-ipfs -- sh -c "rm /data/ipfs/snapshots/snapshot.log" && echo "Removed log file. Exiting container..."; else echo "Error! Log file is empty for some reason..."; false; fi
          volumes:
          - name: lotus-secret-vol
            secret:
              secretName: {{ .Release.Name }}-lotus-secret
              defaultMode: 384  
{{- end }}
{{- end }}
{{- if .Values.persistence.snapshots.automation.creation.enabled }}
---
apiVersion: v1
data:
  snapshot-template.yaml: |
    apiVersion: snapshot.storage.k8s.io/v1beta1
    kind: VolumeSnapshot
    metadata:
      generateName: {{ .Release.Name }}-
      namespace: {{ .Release.Namespace }} 
    spec:
      volumeSnapshotClassName: csi-aws-vsc
      source:
        persistentVolumeClaimName: vol-lotus-{{ .Release.Name }}-lotus-0
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-snapshot-cm

---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-snapshot-creator
spec:
  schedule: {{ .Values.persistence.snapshots.automation.creation.cron | default "0 0 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: "Forbid"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          containers:
          - name: creator
            image: bitnami/kubectl
            args:
            - create
            - -f
            - /tmp/creator/snapshot-template.yaml
            volumeMounts:
            - name: {{ .Release.Name }}-snapshot-cm
              mountPath: "/tmp/creator"
              readOnly: true
          volumes:
          - name: {{ .Release.Name }}-snapshot-cm
            configMap:
              name: {{ .Release.Name }}-snapshot-cm
{{- end }}

{{- if .Values.persistence.snapshots.automation.deletion.enabled }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-snapshot-deleter
spec:
  schedule: {{ .Values.persistence.snapshots.automation.deletion.cron | default "0 1 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: "Forbid"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          containers:
          - name: deleter
            image: bitnami/kubectl
            command:
            - bash
            - -c
            - export ITEMS=$(kubectl get --sort-by .metadata.creationTimestamp -n {{ .Release.Namespace }} --no-headers volumesnapshots | awk '/{{ .Release.Name }}-/{print $1}' | head -n -{{ add .Values.persistence.snapshots.automation.deletion.retention.count }}) && ([ ! -z "$ITEMS" ] && kubectl delete volumesnapshots $ITEMS || true)
{{- end }}
{{- end }}

{{- if .Values.persistence.autoResizer.enabled }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-disk-autoresizer-lotus
spec:
  schedule: {{ .Values.persistence.autoResizer.cron | default "0 0 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          containers:
          - name: resizer
            image: bitnami/kubectl
            command:
            - bash
            - -c
            args:
            - export REPLICAS=$(kubectl get sts -o=jsonpath='{.spec.replicas}' {{ .Release.Name }}-lotus); 
              for ((i=0;i<$REPLICAS;i++)); 
              do 
                until kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin -- df --output=pcent /home/lotus_user/.lotus; do
                  echo "Can't access Lotus node. Sleeping..." && sleep 60s;
                done;
                export USE=$(kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin -- df --output=pcent /home/lotus_user/.lotus | tr -dc '0-9');
                if [[ -z "$USE" ]];
                then
                  echo "Error! Cannot get current disk size for some reason.";
                  false;
                fi;
                if [[ "$USE" -gt "{{ .Values.persistence.autoResizer.increaseThreshold }}" ]]; 
                then
                  export VALUE=$(kubectl get pvc -o=jsonpath='{.spec.resources.requests.storage}' vol-lotus-{{ .Release.Name }}-lotus-${i}); 
                  export OLD_SIZE=$(echo $VALUE | tr -dc '0-9');
                  export NEW_SIZE=$(($OLD_SIZE+$OLD_SIZE*{{ .Values.persistence.autoResizer.increaseStep }}/100)); 
                  export DIMENSION=$(echo $VALUE | tr -dc 'A-z'); 
                  kubectl patch pvc -p='[{"op":"replace","path":"/spec/resources/requests/storage","value":"'${NEW_SIZE}${DIMENSION}'"}]' vol-lotus-{{ .Release.Name }}-lotus-${i} --type='json';
                else echo "Disk is only ${USE}% used, not yet ready to expand. Current threshold is {{ .Values.persistence.autoResizer.increaseThreshold }}%"; 
                fi;
              done;

{{- if .Values.IPFS.enabled }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-disk-autoresizer-ipfs
spec:
  schedule: {{ .Values.persistence.autoResizer.cron | default "0 0 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: "Forbid"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          containers:
          - name: resizer
            image: bitnami/kubectl
            command:
            - bash
            - -c
            args:
            - export REPLICAS=$(kubectl get sts -o=jsonpath='{.spec.replicas}' {{ .Release.Name }}-lotus); 
              for ((i=0;i<$REPLICAS;i++)); 
              do 
                until kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin-ipfs --  df /data/ipfs; do
                  echo "Can't access IPFS node. Sleeping..." && sleep 60s;
                done;
                export USE=$(kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin-ipfs -- df /data/ipfs | awk 'FNR==2{ print $5 }' | tr -dc '0-9');
                if [[ -z "$USE" ]];
                then
                  echo "Error! Cannot get current disk size for some reason.";
                  false;
                fi;
                if [[ "$USE" -gt "{{ .Values.persistence.autoResizer.increaseThreshold }}" ]]; 
                then
                  export VALUE=$(kubectl get pvc -o=jsonpath='{.spec.resources.requests.storage}' vol-ipfs-{{ .Release.Name }}-lotus-${i}); 
                  export OLD_SIZE=$(echo $VALUE | tr -dc '0-9');
                  export NEW_SIZE=$(($OLD_SIZE+$OLD_SIZE*{{ .Values.persistence.autoResizer.increaseStep }}/100)); 
                  export DIMENSION=$(echo $VALUE | tr -dc 'A-z'); 
                  kubectl patch pvc -p='[{"op":"replace","path":"/spec/resources/requests/storage","value":"'${NEW_SIZE}${DIMENSION}'"}]' vol-ipfs-{{ .Release.Name }}-lotus-${i} --type='json';
                else echo "Disk is only ${USE}% used, not yet ready to expand. Current threshold is {{ .Values.persistence.autoResizer.increaseThreshold }}%"; 
                fi;
              done;
{{- end }}

{{- if .Values.Powergate.enabled }}
---
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: {{ .Release.Name }}-disk-autoresizer-powergate
spec:
  schedule: {{ .Values.persistence.autoResizer.cron | default "0 0 * * *" }}
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  concurrencyPolicy: "Forbid"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: "Never"
          serviceAccountName: {{ .Release.Name }}-{{ .Values.serviceAccount.name }}
          containers:
          - name: resizer
            image: bitnami/kubectl
            command:
            - bash
            - -c
            args:
            - export REPLICAS=$(kubectl get sts -o=jsonpath='{.spec.replicas}' {{ .Release.Name }}-lotus); 
              for ((i=0;i<$REPLICAS;i++)); 
              do 
                until kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin-powergate --  df /root/powergate; do
                  echo "Can't access Powergate node. Sleeping..." && sleep 60s;
                done;
                export USE=$(kubectl exec {{ .Release.Name }}-lotus-${i} -c filecoin-powergate -- df /root/powergate | awk 'FNR==2{ print $5 }' | tr -dc '0-9');
                if [[ -z "$USE" ]];
                then
                  echo "Error! Cannot get current disk size for some reason.";
                  false;
                fi;
                if [[ "$USE" -gt "{{ .Values.persistence.autoResizer.increaseThreshold }}" ]]; 
                then
                  export VALUE=$(kubectl get pvc -o=jsonpath='{.spec.resources.requests.storage}' vol-powergate-{{ .Release.Name }}-lotus-${i}); 
                  export OLD_SIZE=$(echo $VALUE | tr -dc '0-9');
                  export NEW_SIZE=$(($OLD_SIZE+$OLD_SIZE*{{ .Values.persistence.autoResizer.increaseStep }}/100)); 
                  export DIMENSION=$(echo $VALUE | tr -dc 'A-z'); 
                  kubectl patch pvc -p='[{"op":"replace","path":"/spec/resources/requests/storage","value":"'${NEW_SIZE}${DIMENSION}'"}]' vol-powergate-{{ .Release.Name }}-lotus-${i} --type='json';
                else echo "Disk is only ${USE}% used, not yet ready to expand. Current threshold is {{ .Values.persistence.autoResizer.increaseThreshold }}%"; 
                fi;
              done;
{{- end }}
{{- end }}
{{- end }}
